---
title: "Einsum"
description: "Einsum Tutorial"
author:
  - name: Felipe Dias
    url: https://fmenegui.github.io/posts/2024-01-02-einsum
date: 01-02-2024
categories: [tutorial, Python, PyTorch] # self-defined categories
citation: 
  url: xxx
image: il.jpg
draft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!
---

# Introduction

Einstein summation notation, commonly denoted as `einsum`,
is a powerful tool for performing optimized matrix operations. It allows
us to avoid using unnecessary for loops and simplifies complex matrix
manipulations, making code more readable and efficient.

Einsum is available in different libraries, such as [Numpy](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html),
[PyTorch](https://pytorch.org/docs/stable/generated/torch.einsum.html) and [Tensorflow](https://www.tensorflow.org/api_docs/python/tf/einsum).

# What is `einsum`?

## Matrix notation

Before diving deeper into `einsum`, letâ€™s briefly review some matrix terminology. Consider a matrix $A$ with 3 rows and 3 columns, represented as follows:

$$
A = \begin{bmatrix}
    a_{11} & a_{12} & a_{13} \\
    a_{21} & a_{22} & a_{23} \\
    a_{31} & a_{32} & a_{33} \\
\end{bmatrix}

$$

This matrix is commonly described as a 3x3 matrix, indicating it has 3 rows and 3 columns.

We can denote the element in the first row and first column as $a_{11}$ and the element in the second row and third column as $a_{23}$.

More generally, we can represent the element in the $i$-th row and $j$-th column as $a_{ij}$. This notation is applicable to any $N$-dimensional tensor. For example, in the case of a one dimensional vector $v$, we can denote the element at position $i$ as $v_i$.

## Intuition

The `einsum` can make two operations over tensors: **multiplication** and **summation**.

We will try to understand the mechanism on how this notation perfom these two operations.

### Multiplication

Letâ€™s consider another 3x3 matrix called $B$. We can create a third matrix, $C$, which contains elements derived from the multiplication of corresponding elements in $A$ and $B$.

For instance, $c_{11} = a_{11} * b_{11}$. In general, $c_{ij} = a_{ij} * b_{ij}$, meaning that the value in the $i$-th row and $j$-th column of $C$ ($c_{ij}$) is obtained by multiplying the $i$-th row and $j$-th column of $A$ ($a_{ij}$) by the $i$-th row and $j$-th column of $B$ ($b_{ij}$).

```python
import numpy as np
A = np.random.random((3, 3))
B = np.random.random((3, 3))
C = np.multiply(A, B)
```

Alternatively, you can achieve the same operation using `einsum` notation:

```python
import numpy as np
A = np.random.random((3, 3))
B = np.random.random((3, 3))
C = np.einsum('ij,ij->ij', A, B)
```

The `einsum` notation structure resembles  $c_{ij} = a_{ij} * b_{ij}$, as shown below:

**cij** = **aij** * **bij**

np.einsum(â€˜**ij**,**ij**->**ij**â€™, **A**,**B**)

This structure follows a specific pattern: first, you specify indices for each of your inputs separated by commas (we have two inputs: `A` and `B`), as in `â€˜ij,ijâ€™` . Then, you add an arrow to indicate the beginning of the output definition (`->`). Finally, you add the indices of the outputs, in this case, `â€˜ijâ€™.` Overall, it appears as `â€˜ij,ij->ijâ€™` .

`einsum` performs the multiplication of corresponding elements ($a_{ij}$ and
$b_{ij}$ in this case).

We can also understand the `einsum` notation by thinking in terms of `for loops`:

```python
C = np.zeros((3,3))
for i in range(3):
    for j in range(3):
        C[i, j] = A[i, j] * B[i, j]
```

Now, letâ€™s consider another example where we multiply the vector $v =
[1, 2, 3]$ element-wise by the vector  $p = [4, 5, 6]$  using `einsum` notation.

Thinking in terms of math notation, we would have $z_i = v_i * p_i$.

Using `einsum`:

```python
v = np.array([1, 2, 3])
p = np.array([4, 5, 6])
z = np.einsum('i,i->i', v, p)
```

In our `for-loop` analogy:

```python
v = np.array([1, 2, 3])
p = np.array([4, 5, 6])
z = np.zeros(3)
for i in range(3):
  z[i] = v[i]*p[i]
```

Now, letâ€™s explore what happens when we use `â€˜i,jâ€™` instead of `â€˜i,iâ€™` in the `einsum` notation for our vectors `v` and `p`:

In this scenario, we introduce one index for vector `v` (denoted as `i`) and another index for vector `p` (denoted as `j`). The einsum operation will perform element-wise multiplication, combing each element of `v` with every element of `p`.

Consider vector `v` with a size of 3 and vector `p` with the same size of
3. When we use `â€˜i,jâ€™` in `einsum`, we end up with **9** possible combinations:

1. $v_0 * p_0 = 4$
2. $v_0 * p_1 = 5$
3. $v_0 * p_2 = 6$
4. $v_1 * p_0 = 8$
5. $v_1 * p_1 = 10$
6. $v_1 * p_2 = 12$
7. $v_2 * p_0 = 12$
8. $v_2 * p_1 = 15$
9. $v_2 * p_2 = 18$

The `einsum` operation organizes these combinations into a 3x3 matrix, where each element $c_{ij}$ corresponds to the product of $v_i$ and $p_j$.

To illustrate this with a for loop analogy:

```python
for i in range(3):
    for j in range(3):
        c[i, j] = v[i] * p[j]
```

In `einsum` notation, we would write:

```python
np.einsum('i,j->ij', v, p)
```

### Summation

As expected by the same, the `einsum` is also able to perform summing operation. It is actually where this operator shines the most.

The sum is performed over an index and the notation to indicate the sum over that index is to ommit it in the output part of the `einsum`.

For example, consider the identify operation, i.e., given a vector
$c_i = a_i$ $~ \forall i$. We can denote this operation using `einsum` using `c = np.einsum(â€˜i->iâ€™, a)`. But we actually want to sum the elements of $a$. How would you do that?

In math notation, the sum is expressed as following: $c = \sum_i
a_i$

Observe that in the math notation there is no index in the output ($c$). In `einsum`, it is the same: `c = np.einsum(â€˜i->â€™, a)`. In this case, `â€˜iâ€™` represent the index of the input ($a_i$), indicates the start of the output definition and the absence of the `â€˜iâ€™` in the output indicates that there is a sum over that index ($\sum_i$)

Analogoly, we could sum all the elements of a matrix $A$ using `einsum` as follows: `np.einsum('ij->', A)`.

Let us try to implement **matrix multiplication** using `einsum`

First, we will recap the concept of **matrix multiplication**. Consider a 2x3 matrix $A$ and a 3x2 matrix $B$:

$$
A = \begin{bmatrix}
    a_{11} & a_{12} & a_{13} \\
    a_{21} & a_{22} & a_{23} \\
\end{bmatrix}

$$

$$
B = \begin{bmatrix}
    b_{11} & b_{12}  \\
    b_{21} & b_{22}  \\
    b_{31} & b_{32}  \\
\end{bmatrix}

$$

The multiplication of $A$ for $B$ is $C$ = $A$ $B$

$$C = \begin{bmatrix}
    a_{11} & a_{12} & a_{13} \\
    a_{21} & a_{22} & a_{23} \\
\end{bmatrix} \begin{bmatrix}
    b_{11} & b_{12}  \\
    b_{21} & b_{22}  \\
    b_{31} & b_{32}  \\
\end{bmatrix}$$

The element of the first row and second column of $C$ is:

$c_{12} = \sum _j a_{1j}b_{j2} = a_{11}b_{11} + a_{12}b_{21} + a_{13}b_{31}$

More generally, the $i$-row row and $j$-column of C is:

$c_{ik} = \sum_j a_{ij} b_{jk}$

We can represent this operation with `einsum` by: `C = np.einsum('ij,jk->ik', A, B)`

<aside>
ðŸ“– To better intuitivelly understand matrix multiplication use: [http://matrixmultiplication.xyz](http://matrixmultiplication.xyz/)

</aside>

## Practice

Let us use our knowledge in `einsum` in practice with some exercises:

- 1) Use `einsum` to make the transpose of a given matrix $A$:
    
    
    The matrix of $A = [a_{ij}]$ has its transpose(commonly denoted as $A^\intercal$
    ) defined as $A^\intercal
     = [a_{ji}]$
    
    So if $C = A^\intercal$, $c_{ij} = a_{ji}$, we could use the following `einsum`
    
    `C = np.einsum('ij->ji', A)`
    
- 2) Calculate the trace of a given square matrix $A$, i.e., the sum of the elements of the diagonal
    
    Given a square matrix (for example a 2x2 matrix):
    
    $$
    A = \begin{bmatrix}
        a_{11} & a_{12}  \\
        a_{21} & a_{22}  \\
    \end{bmatrix}
    $$
    
    The trace of $A$ is the sum of the elements of the diagonal of $A$, i.e., $a_{11} + a_{22}$
    
    In math notation, $Tr(A) = \sum_i a_{ii}$
    
    Using `einsum`, we could do: `np.einsum('iiâ†’â€™, A)`
    
- 3) Calculate the euclidian norm of a vector $v$
    
    The euclidian norm of the vector $v$ is $||v|| = \sqrt{v_0^2 + v_1^2 + \cdots + v_n^2} = \sqrt{\sum _i v_i * v_i}$ 
    
    Using `einsum` we could do: `np.sqrt(p.einsum('i,i->', v, v))`
    
- 4) Calculate the batch matrix multiplication between $A$ and $B$
    
    The matrix multiplication of $A$ ($p \times h$) and $B (h \times u)$, in math notation is defined as follows:
    
    $C = A  B$
    
    $c_{ik} = \sum_j a_{ij} b_{jk}$
    
    Consider a batch of matrices $A$ with size n: $A_{n,p,h}$ and a batch of matrices $B$ also with size $n$: $B_{n,h,u}$ 
    
    How would we multiply $A$ with $B$ ?
    
    We could not use `np.einsum('ij,jk->ik', A, B)` as before ($A$ and $B$ have another dimension).
    
    This batch matrix multiplication (bmm) could be defined in math notation as:
    
    $c_{nik} = \sum_j a_{nij} b_{njk}$
    
    Using `einsum`: `np.einsum('nij,njkâ†’nik, A, B)`
